{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Propensity Score\n",
    "\n",
    "One of the most fundamental features of _RecoGym_ is the ability to collect logs of applying a specific agent in the _RecoGym_ environment. That opens a plethora of opportunities to examine machine learning based on _Counterfactual Risk Minimization_ and compare the result with machine learning based on _Empirical Risk Minimization_.\n",
    "\n",
    "In _[Likelihood Agents](./Likelihood%20Agents.ipynb)_ notebook, some _Agents_ those use different feature sets for _Liklihood_ model have already been analysed. In this notebook, we consider _Inverse Propensity Score_ that is a part of _Counterfactual Risk Minimization_ approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, reco_gym\n",
    "from copy import deepcopy\n",
    "from reco_gym import env_1_args\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "\n",
    "ABTestNumberOfUsers = 10000\n",
    "RandomSeed = 42\n",
    "\n",
    "NumberOfProducts = 10\n",
    "\n",
    "env_1_args['random_seed'] = RandomSeed\n",
    "env_1_args['num_products'] = NumberOfProducts\n",
    "env_1_args['number_of_flips'] = 1\n",
    "env_1_args['num_epochs'] = 100\n",
    "\n",
    "\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(env_1_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = deepcopy(env).generate_logs(ABTestNumberOfUsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "      t  u        z    v    a    c   ps\n",
      "0    0  0  organic  0.0  NaN  NaN  NaN\n",
      "1    1  0   bandit  NaN  3.0  0.0  0.1\n",
      "2    2  0   bandit  NaN  4.0  0.0  0.1\n",
      "3    3  0   bandit  NaN  5.0  0.0  0.1\n",
      "4    0  1  organic  1.0  NaN  NaN  NaN\n",
      "5    1  1   bandit  NaN  2.0  0.0  0.1\n",
      "6    2  1   bandit  NaN  8.0  0.0  0.1\n",
      "7    3  1   bandit  NaN  4.0  0.0  0.1\n",
      "8    4  1  organic  4.0  NaN  NaN  NaN\n",
      "9    5  1  organic  4.0  NaN  NaN  NaN\n",
      "10   6  1  organic  4.0  NaN  NaN  NaN\n",
      "11   7  1  organic  4.0  NaN  NaN  NaN\n",
      "12   8  1  organic  4.0  NaN  NaN  NaN\n",
      "13   9  1  organic  6.0  NaN  NaN  NaN\n",
      "14  10  1  organic  4.0  NaN  NaN  NaN\n",
      "15  11  1  organic  4.0  NaN  NaN  NaN\n",
      "16  12  1  organic  4.0  NaN  NaN  NaN\n",
      "17  13  1  organic  6.0  NaN  NaN  NaN\n",
      "18  14  1  organic  4.0  NaN  NaN  NaN\n",
      "19  15  1  organic  1.0  NaN  NaN  NaN\n",
      "Data Shape:\n",
      " (1005241, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Data:\\n\", data[:20])\n",
    "print(\"Data Shape:\\n\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details regarding the content of the data you shall find in _[Likelihood Agents](./Likelihood%20Agents.ipynb)_ notebook.\n",
    "\n",
    "Please, note that there is a column _**`ps`**_ that is a _Probability Score_ of selecting a particular _`Action`_ by an _`Agent`_. Since the default _`Agent`_ selects an _`Action`_ _**randomly**_ and as there are _`10`_ different _`Actions`_ in overall, the _Probability Score_ is always $\\frac{1}{10}$ = _`0.1`_ for all _Bandit_ _`Events`_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification Models\n",
    "\n",
    "All models analised below use _Multi-Class Classification_ where for a provided set of features, the models predict an _`Action`_.\n",
    "\n",
    "That approach differs to what was used in _Logistic Regression_ for _Emirical Risk Minimization_ where an _`Action`_ was a part of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def build_train_data(\n",
    "    data,\n",
    "    weight_history_function = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Build Train Data\n",
    "\n",
    "        Parameters:\n",
    "            data: offline experiment logs\n",
    "                the data contains both Organic and Bandit Events\n",
    "            weight_history_function(function): weight functions that assigns an appropriate weight\n",
    "                for View in Organic Event for a certain Product\n",
    "\n",
    "        Returns:\n",
    "            :(features, outs)\n",
    "    \"\"\"\n",
    "    num_products = int(data.v.max() + 1)\n",
    "    number_of_users = int(data.u.max()) + 1\n",
    "\n",
    "    features = []\n",
    "    actions = []\n",
    "    pss = []\n",
    "    clicks = []\n",
    "\n",
    "    for user_id in range(number_of_users):\n",
    "        views = np.zeros((0, num_products))\n",
    "        for _, user_datum in data[data['u'] == user_id].iterrows():\n",
    "            if user_datum['z'] == 'organic':\n",
    "                assert (math.isnan(user_datum['a']))\n",
    "                assert (math.isnan(user_datum['c']))\n",
    "                assert (not math.isnan(user_datum['v']))\n",
    "\n",
    "                view = int(user_datum['v'])\n",
    "\n",
    "                tmp_view = np.zeros(num_products)\n",
    "                tmp_view[view] = 1\n",
    "\n",
    "                # Append the latest view at the beginning of all views.\n",
    "                views = np.append(tmp_view[np.newaxis, :], views, axis = 0)\n",
    "            else:\n",
    "                assert (user_datum['z'] == 'bandit')\n",
    "                assert (not math.isnan(user_datum['a']))\n",
    "                assert (not math.isnan(user_datum['c']))\n",
    "                assert (math.isnan(user_datum['v']))\n",
    "\n",
    "                action = int(user_datum['a'])\n",
    "                click = int(user_datum['c'])\n",
    "                ps = int(user_datum['ps'])\n",
    "\n",
    "                if weight_history_function is None:\n",
    "                    train_views = views\n",
    "                else:\n",
    "                    history = np.array(range(views.shape[0])).reshape(views.shape[0], 1)\n",
    "                    weights = weight_history_function(history)\n",
    "                    train_views = views * weights\n",
    "\n",
    "                feature = np.sum(train_views, axis = 0)\n",
    "\n",
    "                features.append(feature)\n",
    "                actions.append(action)\n",
    "                clicks.append(click)\n",
    "                pss.append(ps)\n",
    "\n",
    "    return np.array(features), np.array(actions), np.array(clicks), np.array(pss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Wieghted Training Data\n",
    "\n",
    "The model is based on _Logistic Regression_ of multiple classes (different _`Actions`_) where during training each feature set will be weighted. The weight for $i^th$ feature set is actually _Inverse Propensity Score_ that is calculated as follows:\n",
    "\n",
    "\n",
    "\n",
    "$$ w_i \\equiv ips_i = \\frac{\\delta_i}{p_i} $$\n",
    "\n",
    "* $\n",
    "\\delta_i =\n",
    "\\begin{cases}\n",
    "    1,& \\text{if there is a click}\\\\\n",
    "    0,& \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "* $p_i$ is a _Probability Score_ that is for the case of _Random_ _`Agent`_ is _**always**_ _`0.1`_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features01, train_actions01, train_clicks01, train_pss01 = build_train_data(data)\n",
    "\n",
    "random_ps = 1.0 / NumberOfProducts\n",
    "train_weights01 = train_clicks01 / random_ps\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_weighted01 = LogisticRegression(\n",
    "    solver = 'lbfgs',\n",
    "    max_iter = 5000,\n",
    "    multi_class='multinomial',\n",
    "    random_state = RandomSeed\n",
    ")\n",
    "\n",
    "lr_weighted01 = logreg_weighted01.fit(train_features01, train_actions01, train_weights01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_weighted01.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the model works if in _`Views`_ thare are _`10`_s for each _`Product`_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_views01 = np.zeros((NumberOfProducts, NumberOfProducts))\n",
    "for product_id in range(NumberOfProducts):\n",
    "    test_views01[product_id, product_id] = 10\n",
    "print(\"Predicted Action: \", lr_weighted01.predict(test_views01))\n",
    "print(\"Predicted Probability of Actions: \", lr_weighted01.predict_proba(test_views01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, only _`Products`_ _`4`_, _`5`_, and _`9`_ correlate with amount of appropriate _`Views`_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a graph to reveal the correlation of matched products with the appropriate amount of _`Views`_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_product_ids = np.array(range(NumberOfProducts))\n",
    "inc_matrix = np.eye(NumberOfProducts)\n",
    "views = np.zeros((NumberOfProducts, NumberOfProducts))\n",
    "counts = []\n",
    "matcheses = []\n",
    "for count in range(200):\n",
    "    views += inc_matrix\n",
    "    predictions = lr_weighted01.predict(views)\n",
    "    matches = np.sum(predictions == all_product_ids)\n",
    "    matcheses.append(matches)\n",
    "    counts.append(count)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(counts, matcheses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will build a more complex model with product crossfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "logreg_weighted02 = LogisticRegression(\n",
    "    solver = 'lbfgs',\n",
    "    max_iter = 10000,\n",
    "    multi_class='multinomial',\n",
    "    random_state = RandomSeed\n",
    ")\n",
    "poly2 = PolynomialFeatures(2)\n",
    "lr_weighted02 = logreg_weighted02.fit(poly2.fit_transform(train_features01), train_actions01, train_weights01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = np.zeros((NumberOfProducts, NumberOfProducts))\n",
    "counts = []\n",
    "matcheses = []\n",
    "for count in range(200):\n",
    "    views += inc_matrix\n",
    "    predictions = lr_weighted02.predict(poly2.fit_transform(views))\n",
    "    matches = np.sum(predictions == all_product_ids)\n",
    "    matcheses.append(matches)\n",
    "    counts.append(count)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(counts, matcheses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, it is not clear yet how the model behavies and what kind of performance in _Click-Through Rate_ shall we expect. To reveal that, we bench the _`Agent`_ that uses the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis with Inversed Propensity Score\n",
    "\n",
    "So far, _[scikit-learn](https://scikit-learn.org)_ was used to create models. This time, we are going to build a more complicated model based on a _Neural Network_. The model implements a new hypothesis $h_c(A=a|V)$ that for given _`Views`_ finds _the probability of the click_ for an _`Action`_ $a$. The value of that function is _Probability Score_ that is stored by _RecoGym_ in offline logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, we will use _[PyTorch](https://pytorch.org)_ to train models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "\\DeclareMathOperator{\\Var}{Var}\n",
    "\\hat{R}^{M}(h_c) = \\frac{1}{n} \\sum_{i}^{N}\\delta_{i} \\min \\left \\{ M, \\frac{h_c(A = a_i|V_i)}{p_i} \\right \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_i = \\delta_i \\min \\left \\{ M, \\frac{h_c(A = a_i|V_i)}{p_i} \\right \\}, \\bar{u} = \\frac{1}{N} \\sum_{i}^{N} u_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Var_{h_c}(u) = \\frac{1}{N - 1} \\sum_{i}^{N} \\left( u_i - \\bar{u} \\right)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{h}_c^{CRM} = \\argmin_{h \\in \\mathcal{H}} \\left \\{ \\hat{R}^{M}(h_c) + \\lambda \\sqrt{\\frac{\\Var_{h_c}(u)}{N}} \\right \\} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the new model, $\\hat{h}_c^{CRM}$ is to be minimised, where:\n",
    "* $\\hat{h}_c^{CRM}$ is a new hypothesis that based on _Counterfactual Risk Minimization_\n",
    "* $N$ is the number of training samples\n",
    "* $p_i$ is a _Propensity Score_ or a probability of applying the default hypothesis $h_0$; in our case, it is always $\\frac{1}{10}=0.1$\n",
    "* $a_i$ is an _`Action`_ provided in the ith training sample\n",
    "* $V_i$ _`Views`_ at ith training sample\n",
    "\n",
    "In the new model, we also use a regularisation defined as: $\\lambda \\sqrt{\\frac{\\Var_{h_c}(u)}{N}}$.\n",
    "\n",
    "The new hypothesis is implemented in a neural network.\n",
    "The network has the following topology:\n",
    "* _`2`_ sigmoid layers\n",
    "* _`1`_ _softmax_ layer\n",
    "\n",
    "More details about the structure of the NN, you shall find in _`NeuralNet`_ class.\n",
    "\n",
    "The model used here was motivated by the article _[arXiv:1502.02362](https://arxiv.org/pdf/1502.02362.pdf)_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Agent Performance\n",
    "\n",
    "Let's compare the performance of models with _IPS & Multi-Class Logistic Regression_ vs _IPS & Hypothesis implemented in Neural Network_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, reco_gym\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd\n",
    "from reco_gym import env_1_args, Configuration\n",
    "from agents import LogregPolyAgent, logreg_poly_args\n",
    "from agents import LogregMulticlassIpsAgent, logreg_multiclass_ips_args\n",
    "from agents import NnIpsAgent, nn_ips_args\n",
    "\n",
    "NumberOfflineUsers = 10000\n",
    "NumberTestUsers = 1000\n",
    "\n",
    "test_env = {\n",
    "    **env_1_args,\n",
    "    **{\n",
    "        'random_seed': RandomSeed,\n",
    "    }\n",
    "}\n",
    "\n",
    "env = gym.make('reco-gym-v1')\n",
    "env.init_gym(test_env)\n",
    "\n",
    "\n",
    "def agent_performance(\n",
    "    agent,\n",
    "    user_per_step = 1000,\n",
    "    user_steps = 10,\n",
    "    test_users = NumberTestUsers\n",
    "):\n",
    "    steps = []\n",
    "    data = {\n",
    "        '0.025': [],\n",
    "        'mean': [],\n",
    "        '0.975': [],\n",
    "    }\n",
    "    for step in range(user_steps):\n",
    "        number_of_offline_users = (step + 1) * user_per_step\n",
    "        stats = reco_gym.test_agent(\n",
    "            deepcopy(env),\n",
    "            agent,\n",
    "            number_of_offline_users,\n",
    "            test_users\n",
    "        )\n",
    "        steps.append(number_of_offline_users)\n",
    "        data['mean'].append(stats[0])\n",
    "        data['0.025'].append(stats[1])\n",
    "        data['0.975'].append(stats[2])\n",
    "    \n",
    "    figs, axs = plt.subplots(\n",
    "        1,\n",
    "        1,\n",
    "        figsize = (16, 8)\n",
    "    )\n",
    "    axs.fill_between(\n",
    "        range(len(data['mean'])),\n",
    "        data['0.975'],\n",
    "        data['0.025'],\n",
    "        alpha=.05\n",
    "    )\n",
    "    axs.plot(steps, data['mean'])\n",
    "    axs.set_xlabel('Samples #')\n",
    "    axs.set_ylabel('CTR')\n",
    "    axs.legend(['$\\mu$'])\n",
    "    plt.show()\n",
    "    \n",
    "    return pd.DataFrame().from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model: _Logistic Regression_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat00 = agent_performance(\n",
    "    LogregPolyAgent(Configuration({\n",
    "        **logreg_poly_args,\n",
    "        **test_env,\n",
    "        'with_ips': False,\n",
    "    }))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CTR of Logistic Regression:\\n\", stat00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model: _Logistic Regression with Inverse Propensity Score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat01 = agent_performance(\n",
    "    LogregPolyAgent(Configuration({\n",
    "        **logreg_poly_args,\n",
    "        **test_env,\n",
    "        'with_ips': True,\n",
    "    }))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CTR of Logistic Regression & IPS:\\n\", stat01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model: _Multiclass Logistic Regression with Inverse Propensity Score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat02 = agent_performance(\n",
    "    LogregMulticlassIpsAgent(Configuration({\n",
    "        **logreg_multiclass_ips_args,\n",
    "        **test_env,\n",
    "    }))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CTR of Multi-Class Logistic Regression & IPS:\\n\", stat02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model: _Hypothesis by Neural Network with Inverse Propensity Score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat03 = agent_performance(\n",
    "    NnIpsAgent(Configuration({\n",
    "        **nn_ips_args,\n",
    "        **test_env,\n",
    "    }))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CTR of Hypothesis by Neural Network & IPS:\\n\", stat03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
